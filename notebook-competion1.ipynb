{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Cette page doit être lancée sur Kaggle, depuis la compétition pour avoir accès aux données. Sinon vous devez récupérer les données séparément.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib.pyplot as plt# data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfrom IPython.display import FileLink\nSEED = 25\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\nOUTPUT_DIR = '/kaggle/working/outputs'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-14T13:36:11.518695Z","iopub.execute_input":"2023-05-14T13:36:11.519313Z","iopub.status.idle":"2023-05-14T13:36:26.886912Z","shell.execute_reply.started":"2023-05-14T13:36:11.519272Z","shell.execute_reply":"2023-05-14T13:36:26.885737Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/outputs","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:36:26.889157Z","iopub.execute_input":"2023-05-14T13:36:26.890555Z","iopub.status.idle":"2023-05-14T13:36:27.906614Z","shell.execute_reply.started":"2023-05-14T13:36:26.890508Z","shell.execute_reply":"2023-05-14T13:36:27.905212Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Lire les images\n\ncf https://keras.io/api/data_loading/image/ et \nhttps://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory","metadata":{}},{"cell_type":"code","source":"directory_path = \"/kaggle/input/navires-2023-la-mano/ships16x24/ships_16x24_10cat/data\"\nds_train, ds_val = tf.keras.utils.image_dataset_from_directory(directory_path, labels='inferred',subset='both',shuffle=True,validation_split=0.2,seed=SEED,image_size=(16,24))\n","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:36:27.909444Z","iopub.execute_input":"2023-05-14T13:36:27.910765Z","iopub.status.idle":"2023-05-14T13:37:13.842014Z","shell.execute_reply.started":"2023-05-14T13:36:27.910717Z","shell.execute_reply":"2023-05-14T13:37:13.840942Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Found 35515 files belonging to 10 classes.\nUsing 28412 files for training.\nUsing 7103 files for validation.\n","output_type":"stream"}]},{"cell_type":"code","source":"ds_train","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:37:13.845265Z","iopub.execute_input":"2023-05-14T13:37:13.845642Z","iopub.status.idle":"2023-05-14T13:37:13.854739Z","shell.execute_reply.started":"2023-05-14T13:37:13.845602Z","shell.execute_reply":"2023-05-14T13:37:13.853715Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<BatchDataset element_spec=(TensorSpec(shape=(None, 16, 24, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Résultat à soumettre","metadata":{"execution":{"iopub.status.busy":"2022-05-02T16:53:05.081093Z","iopub.execute_input":"2022-05-02T16:53:05.081503Z","iopub.status.idle":"2022-05-02T16:53:05.087029Z","shell.execute_reply.started":"2022-05-02T16:53:05.081444Z","shell.execute_reply":"2022-05-02T16:53:05.085976Z"}}},{"cell_type":"code","source":"class CheckValidationLossCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, patience=0):\n        self.patience = patience\n        self.counter = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        val_loss = logs.get('val_loss')\n        train_loss = logs.get('loss')\n        if val_loss < train_loss:\n            self.counter = 0\n            print(f\"\\nValidation loss ({val_loss:.4f}) is less than training loss ({train_loss:.4f})\")\n        else:\n            self.counter +=1\n            \n        if(self.counter == self.patience):\n            self.model.stop_training = True","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:37:13.856600Z","iopub.execute_input":"2023-05-14T13:37:13.857396Z","iopub.status.idle":"2023-05-14T13:37:13.867413Z","shell.execute_reply.started":"2023-05-14T13:37:13.857358Z","shell.execute_reply":"2023-05-14T13:37:13.866203Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def test_phase(model):\n    file_name = f'submission.csv'\n    X_test = np.load('/kaggle/input/navires-2023-la-mano/test.npy', allow_pickle=True)\n    X_test = X_test.astype('float32')\n    res = model.predict(X_test).argmax(axis=1)\n    df = pd.DataFrame({\"Category\":res})\n    df.to_csv(os.path.join(file_name), index_label=\"Id\")\n\ndef save_history(model,history):\n    history_df = pd.DataFrame(history.history)\n    hist_file_name = f\"history_{model.name}.csv\"\n    history_df.to_csv(os.path.join(hist_file_name))\n\ndef save_model(model):\n    model_file_name = f\"{model.name}.h5\"\n    model.save(os.path.join(model_file_name))","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:37:13.868991Z","iopub.execute_input":"2023-05-14T13:37:13.869458Z","iopub.status.idle":"2023-05-14T13:37:13.882006Z","shell.execute_reply.started":"2023-05-14T13:37:13.869411Z","shell.execute_reply":"2023-05-14T13:37:13.880935Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D,MaxPooling2D, MaxPooling2D, Dropout,Flatten, Dense, BatchNormalization, Rescaling, LeakyReLU\nfrom tensorflow.keras.models import load_model\n\"\"\"\ndef build_model() -> Model:\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(64, 3, activation='relu', input_shape=(16, 24, 3)),\n        tf.keras.layers.MaxPooling2D(2),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.BatchNormalization(),\n        \n        tf.keras.layers.Conv2D(32, 3, activation='relu',padding='same'),\n        tf.keras.layers.MaxPooling2D(2),\n        tf.keras.layers.Dropout(0.5),\n         tf.keras.layers.BatchNormalization(),\n        \n        tf.keras.layers.Conv2D(16, 3, activation='relu',padding='same'),\n        tf.keras.layers.MaxPooling2D(2),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.BatchNormalization(),\n        \n        tf.keras.layers.Flatten(),\n        \n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dropout(0.6),\n        tf.keras.layers.Dense(10,activation='softmax')\n    ])\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=['accuracy'])\n    \n    return model\"\"\"\ndef build_model(input_shape=(16, 24, 3), num_classes=10) -> Model:\n    model = Sequential([\n        Conv2D(32, (3, 3), padding='same', input_shape=input_shape,activation='relu'),\n        LeakyReLU(alpha=0.1),\n        Conv2D(64, (3, 3), padding='same',activation='relu'),\n        LeakyReLU(alpha=0.1),\n        BatchNormalization(),\n        MaxPooling2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        Conv2D(128, (3, 3), padding='same',activation='relu'),\n        LeakyReLU(alpha=0.1),\n        BatchNormalization(),\n        MaxPooling2D(pool_size=(2, 2)),\n        Dropout(0.5),\n        Flatten(),\n        Dense(256,activation='relu'),\n        LeakyReLU(alpha=0.3),\n        BatchNormalization(),\n        Dropout(0.65),\n        Dense(num_classes, activation='softmax')\n    ])\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n\n    return model\nmodel = build_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:53:15.302083Z","iopub.execute_input":"2023-05-14T13:53:15.302847Z","iopub.status.idle":"2023-05-14T13:53:15.521242Z","shell.execute_reply.started":"2023-05-14T13:53:15.302807Z","shell.execute_reply":"2023-05-14T13:53:15.520399Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_3 (Conv2D)           (None, 16, 24, 32)        896       \n                                                                 \n leaky_re_lu_4 (LeakyReLU)   (None, 16, 24, 32)        0         \n                                                                 \n batch_normalization_4 (Batc  (None, 16, 24, 32)       128       \n hNormalization)                                                 \n                                                                 \n conv2d_4 (Conv2D)           (None, 16, 24, 64)        18496     \n                                                                 \n leaky_re_lu_5 (LeakyReLU)   (None, 16, 24, 64)        0         \n                                                                 \n batch_normalization_5 (Batc  (None, 16, 24, 64)       256       \n hNormalization)                                                 \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 8, 12, 64)        0         \n 2D)                                                             \n                                                                 \n dropout_3 (Dropout)         (None, 8, 12, 64)         0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 8, 12, 128)        73856     \n                                                                 \n leaky_re_lu_6 (LeakyReLU)   (None, 8, 12, 128)        0         \n                                                                 \n batch_normalization_6 (Batc  (None, 8, 12, 128)       512       \n hNormalization)                                                 \n                                                                 \n max_pooling2d_3 (MaxPooling  (None, 4, 6, 128)        0         \n 2D)                                                             \n                                                                 \n dropout_4 (Dropout)         (None, 4, 6, 128)         0         \n                                                                 \n flatten_1 (Flatten)         (None, 3072)              0         \n                                                                 \n dense_2 (Dense)             (None, 512)               1573376   \n                                                                 \n leaky_re_lu_7 (LeakyReLU)   (None, 512)               0         \n                                                                 \n batch_normalization_7 (Batc  (None, 512)              2048      \n hNormalization)                                                 \n                                                                 \n dropout_5 (Dropout)         (None, 512)               0         \n                                                                 \n dense_3 (Dense)             (None, 10)                5130      \n                                                                 \n=================================================================\nTotal params: 1,674,698\nTrainable params: 1,673,226\nNon-trainable params: 1,472\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_and_test(epochs=50,model=None):\n    callbacks = []\n    #callbacks.append(CheckValidationLossCallback(patience=3))\n    #callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5))\n    if model is None:\n        model = build_model() \n    history = model.fit(ds_train,epochs=epochs,validation_data=ds_val,callbacks=callbacks,batch_size = 250)\n    test_phase(model)\n    save_model(model)\n    save_history(model,history)\n    return history\n\n#model = load_model('/kaggle/input/model-bast/sequential_18.h5')\nhistory = train_and_test(epochs=30,model=model)\ntest_phase(model)\nsave_model(model)\n\npd.DataFrame(history.history).plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:53:17.822631Z","iopub.execute_input":"2023-05-14T13:53:17.823438Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/75\n","output_type":"stream"},{"name":"stderr","text":"2023-05-14 13:53:19.187999: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/dropout_3/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"888/888 [==============================] - 42s 43ms/step - loss: 2.1633 - accuracy: 0.3628 - val_loss: 1.4723 - val_accuracy: 0.4981\n","output_type":"stream"}]},{"cell_type":"code","source":"test_phase(model)\nFileLink('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:49:16.714870Z","iopub.execute_input":"2023-05-14T13:49:16.716969Z","iopub.status.idle":"2023-05-14T13:49:17.134573Z","shell.execute_reply.started":"2023-05-14T13:49:16.716935Z","shell.execute_reply":"2023-05-14T13:49:17.133574Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"116/116 [==============================] - 0s 2ms/step\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"!rm -rf ships32/","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:49:17.139446Z","iopub.execute_input":"2023-05-14T13:49:17.139771Z","iopub.status.idle":"2023-05-14T13:49:18.268384Z","shell.execute_reply.started":"2023-05-14T13:49:17.139739Z","shell.execute_reply":"2023-05-14T13:49:18.266968Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(\"Nombre de couches : \", len(model.layers))","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:49:18.270393Z","iopub.execute_input":"2023-05-14T13:49:18.271077Z","iopub.status.idle":"2023-05-14T13:49:18.278011Z","shell.execute_reply.started":"2023-05-14T13:49:18.271030Z","shell.execute_reply":"2023-05-14T13:49:18.276842Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Nombre de couches :  19\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}}]}